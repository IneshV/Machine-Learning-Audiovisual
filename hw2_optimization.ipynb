{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 2\n",
        "\n",
        "Implement early stopping by halting the training when the difference in consecutive loss values is below 0.001 for five consecutive epochs.\n",
        "\n",
        "During the training phase, the object `loss` already contains the loss for the current and all previous iterations. So every iteration of the loop, you will want to check whether the difference between the current loss and the loss in the previous step is smaller than 0.001. Then you need to find a way of tracking whether this happens on 5 consecutive steps, and if so, use `break` to halt the loop.\n",
        "\n",
        "At what iteration does your training loop with early stopping stop? How close are the betas to what they would be with the full 5000 iterations?\n",
        "\n",
        "The homework is due on Thursday 9/29, at 11:59pm.\n"
      ],
      "metadata": {
        "id": "0lliBUMBXqW9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kOqkRRqEXFy3",
        "outputId": "33adccaf-23d8-4d35-b55b-fc544a9900ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Czz6iLAfIS58b5kAhKjmVzZJDuPnDgTF\n",
            "To: /content/data.csv\n",
            "\r  0% 0.00/1.46k [00:00<?, ?B/s]\r100% 1.46k/1.46k [00:00<00:00, 1.43MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Don't change anything in this cell\n",
        "\n",
        "!gdown https://drive.google.com/uc?id=1Czz6iLAfIS58b5kAhKjmVzZJDuPnDgTF\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "data = pd.read_csv('data.csv', header = None, names = ['X','y'])\n",
        "\n",
        "X = data.X.values\n",
        "y = data.y.values\n",
        "\n",
        "# Regression model\n",
        "def regress(X, beta):\n",
        "    f = beta[0] + beta[1]*X\n",
        "    return f\n",
        "\n",
        "# Mean squared error loss\n",
        "def computeLoss(X, y, beta): \n",
        "    # number of samples\n",
        "    m = X.shape[0]\n",
        "    # sum of squared errors\n",
        "    sqe = np.sum((regress(X, beta)-y)**2)\n",
        "    # mean squared error\n",
        "    msqe = sqe/(2*m)\n",
        "    return msqe\n",
        "\n",
        "def computeGrad(X, y, beta):\n",
        "    m = X.shape[0]\n",
        "    # derivative of the loss w.r.t. model bias b, i.e. beta 0\n",
        "    dL_db = (np.sum(regress(X, beta)-y))/m \n",
        "    # derivative of the loss w.r.t model weights w, i.e. beta 1\n",
        "    dL_dw = (np.sum((regress(X, beta)-y)*X))/m\n",
        "    # full gradient\n",
        "    gradient = (dL_db, dL_dw) \n",
        "    return gradient"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify this cell to implement early stopping\n",
        "\n",
        "# Convert X and y from the data frame to numpy arrays again\n",
        "# (just in case we overwrote them somewhere)\n",
        "X = data.X.values\n",
        "y = data.y.values\n",
        "\n",
        "# Initalize bias at 0 and weights at 1\n",
        "b = np.array([0])\n",
        "w = np.array([1])\n",
        "beta = (b, w)\n",
        "\n",
        "# Training loop\n",
        "L = computeLoss(X, y, beta)\n",
        "print(\"-1 L = {0}\".format(L))\n",
        "alpha = 0.01 # step size coefficient\n",
        "n_epoch = 5000 # number of epochs (full passes through the dataset)\n",
        "L_best = L\n",
        "loss = L.copy()\n",
        "beta0s = b.copy()\n",
        "beta1s = w.copy()\n",
        "\n",
        "Stopper = 0\n",
        "Losslast = 0\n",
        "for i in range(n_epoch):\n",
        "    \n",
        "\n",
        "\n",
        "    dL_db, dL_dw = computeGrad(X, y, beta)\n",
        "    b = beta[0]\n",
        "    w = beta[1]\n",
        "    # update rules\n",
        "    newbeta0 = b-alpha*dL_db\n",
        "    newbeta1 = w-alpha*dL_dw\n",
        "    # override the beta\n",
        "    beta = (newbeta0, newbeta1)\n",
        "    # track our loss after performing a single step\n",
        "    L = computeLoss(X, y, beta) \n",
        "    loss = np.append(loss, L)\n",
        "    beta0s = np.append(beta0s, beta[0])\n",
        "    beta1s = np.append(beta1s, beta[1])\n",
        "    if abs(L-Losslast)<0.001:\n",
        "      Stopper +=1\n",
        "    else:\n",
        "      Stopper = 0\n",
        "    if Stopper == 5:\n",
        "      print(\"\\nIn the case of 5,000: Beta 0:-3.89522094 Beta 1: 1.19297739\",\"\\n\\nStopped at iteration\",i,)\n",
        "\n",
        "      break\n",
        "\n",
        "    Losslast = L\n",
        "\n",
        "    # Print information about the training progress every 100 epochs\n",
        "    if i % 100 == 0:\n",
        "      print(\"\\nEpoch = \", i)\n",
        "      print(\"Loss = \", L)\n",
        "      print(\"weight (b1) = \", w)\n",
        "      print(\"bias (b0) = \", b )\n",
        "      print(\"beta0 =\", newbeta0)\n",
        "      print(\"beta1 =\", newbeta1)\n",
        "\n"
      ],
      "metadata": {
        "id": "J3R_B88IXRBX",
        "outputId": "6a4dab07-b0ce-48b8-f4e6-deb2a047e460",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-1 L = 7.445855542929897\n",
            "\n",
            "Epoch =  0\n",
            "Loss =  5.8903978572788285\n",
            "weight (b1) =  [1]\n",
            "bias (b0) =  [0]\n",
            "beta0 = [-0.02320665]\n",
            "beta1 = [0.83924907]\n",
            "\n",
            "Epoch =  100\n",
            "Loss =  5.426983895839554\n",
            "weight (b1) =  [0.86792364]\n",
            "bias (b0) =  [-0.6595947]\n",
            "beta0 = [-0.66542824]\n",
            "beta1 = [0.86850968]\n",
            "\n",
            "Epoch =  200\n",
            "Loss =  5.139213127515016\n",
            "weight (b1) =  [0.92159358]\n",
            "bias (b0) =  [-1.19383209]\n",
            "beta0 = [-1.19870262]\n",
            "beta1 = [0.92208288]\n",
            "\n",
            "Epoch =  300\n",
            "Loss =  4.938611752614805\n",
            "weight (b1) =  [0.96640356]\n",
            "bias (b0) =  [-1.63987629]\n",
            "beta0 = [-1.64394278]\n",
            "beta1 = [0.96681208]\n",
            "\n",
            "Epoch =  400\n",
            "Loss =  4.798775062994989\n",
            "weight (b1) =  [1.0038162]\n",
            "bias (b0) =  [-2.01228644]\n",
            "beta0 = [-2.01568162]\n",
            "beta1 = [1.00415728]\n",
            "\n",
            "In the case of 5,000: Beta 0:-3.89522094 Beta 1: 1.19297739 \n",
            "\n",
            "Stopped at iteration 446\n"
          ]
        }
      ]
    }
  ]
}