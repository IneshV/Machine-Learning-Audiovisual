{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "P21DmAUl0RWv",
        "vmhAYMLYW2qu",
        "5I1eCbS6WsXo"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vf_S4bH9XQGD"
      },
      "source": [
        "# Audio Day 4 - Audio Features"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mel Frequency Cepstral Coefficients (MFCCs)\n",
        "\n",
        "MFCCs are by far the most commonly used audio feature in machine learning. You can build good models with nothing but MFCCs."
      ],
      "metadata": {
        "id": "4AyrzODHS4fj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw1-QC27XMOi"
      },
      "source": [
        "### Mel scale\n",
        "\n",
        "The Mel-scale is a perceptual scale of pitch judged by listeners to be equidistant.\n",
        "\n",
        "It aims to mimic the non-linear human ear perception of sound. \n",
        "\n",
        "The name mel comes from the word melody to indicate that the scale is based on pitch comparisons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afVss7LOEtR9"
      },
      "source": [
        "The frequency-mel conversion is based on results of experiments on human subjects, so there is no objectively correct formula.\n",
        "\n",
        "Example formula to convert from frequency to mel:\n",
        "\n",
        "$m = 2595 \\log_{10}(1+\\frac{f}{700})$\n",
        "\n",
        "Mel to frequency:\n",
        "\n",
        "$f = 700(10^{\\frac{m}{2595}}-1)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCDG-7jgDB1q"
      },
      "source": [
        "![](https://upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Mel-Hz_plot.svg/1920px-Mel-Hz_plot.svg.png)\n",
        "\n",
        "Let's take a look at how a linear scale spectrogram translates to a mel scale spectrogram in Audacity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-Y8Ld-WXNnT"
      },
      "source": [
        "### Mel Frequency Cepstral Coefficients (MFCCs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDftvzZIFWSb"
      },
      "source": [
        "Like formants, MFCCs are another way of representing information about the deformation of the human vocal tract. They are especially useful for machine learning.\n",
        "\n",
        "To calculate the MFCCs, we do the following:\n",
        "\n",
        "1. Divide the signal into frames\n",
        "2. Apply the Fourier Transform to create the spectrum of each frame\n",
        "3. Apply a mel filterbank\n",
        "4. Take the log of filterbank\n",
        "5. Apply the discrete cosine transform\n",
        "\n",
        "Depending on how many filters are used, we get up to 40 cepstral coefficents for each frame."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9ZB9KFcDYok"
      },
      "source": [
        "Mel filterbank\n",
        "\n",
        "Each filter in a Mel filterbank has a triangular shape and can be a applied to a spectrum  by multiplying the amplitude by 1 at the center of the filter, all the way down to 0 at the edges of the filter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItfIW468DaAk"
      },
      "source": [
        "![](https://haythamfayek.com/assets/posts/post1/mel_filters.jpg)\n",
        "Source: [Haytham Fayek](https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7m6gkg9JCgHc"
      },
      "source": [
        "![](http://practicalcryptography.com/media/miscellaneous/files/mel_filterbank_example.png)\n",
        "\n",
        "Source: [James Lyons](http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "madz620WWc8R"
      },
      "source": [
        "MFCCs are widely used in speech recognition, musical genre classification, speaker clustering, emotion recognition, etc.\n",
        "\n",
        "The first 12-20 MFCCs are often considered to carry enough discriminating information in the context of various classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpScSSFeY5Xv"
      },
      "source": [
        "### Extracting MFCCs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASJ8_KcOxis2"
      },
      "source": [
        "In addition to representing audio signals in Python, our primary use of Librosa is [feature extraction](https://librosa.org/doc/latest/feature.html)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's work with the [i] again\n",
        "!wget https://upload.wikimedia.org/wikipedia/commons/9/91/Close_front_unrounded_vowel.ogg\n",
        "!ffmpeg -i \"Close_front_unrounded_vowel.ogg\" \"i.wav\" -y"
      ],
      "metadata": {
        "id": "Zfcv-Ij5NwwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data\n",
        "import librosa\n",
        "y, sr = librosa.load(\"i.wav\", mono = True)"
      ],
      "metadata": {
        "id": "XppVocPoN18c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRuRs3KHEegP"
      },
      "source": [
        "mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)\n",
        "print(mfcc.shape)\n",
        "print(mfcc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa.display\n",
        "librosa.display.specshow(mfcc)"
      ],
      "metadata": {
        "id": "vL7cWh9J7eyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's investigate what's going on under the hood and look at the Librosa documentation and source code for [mfcc](https://librosa.org/doc/latest/generated/librosa.feature.mfcc.html) and [melspectrogram](https://librosa.org/doc/latest/generated/librosa.feature.melspectrogram.html)."
      ],
      "metadata": {
        "id": "DcfKj3RYPtdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Let's start with a regular spectrogram\n",
        "import numpy as np\n",
        "import librosa.display\n",
        "D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
        "librosa.display.specshow(D, y_axis = 'linear')"
      ],
      "metadata": {
        "id": "Ka8SypXqZCPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Produce a linear transformation matrix \n",
        "# to project FFT bins onto Mel-frequency bins\n",
        "n_fft = 2048\n",
        "mel_basis = librosa.filters.mel(sr=sr, n_fft=n_fft)"
      ],
      "metadata": {
        "id": "I81Xmn8aRj8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Illustrate the transformation matrix\n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots()\n",
        "img = librosa.display.specshow(mel_basis, x_axis='linear', ax=ax)\n",
        "ax.set(ylabel='Mel filter', title='Mel filter bank')\n",
        "fig.colorbar(img, ax=ax)"
      ],
      "metadata": {
        "id": "9T5zFEDVTXip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform the spectrogram to mel scale\n",
        "D_mel = np.einsum(\"...ft,mf->...mt\", D, mel_basis, optimize=True)\n",
        "# and take a look at it\n",
        "librosa.display.specshow(D_mel, x_axis='linear')"
      ],
      "metadata": {
        "id": "yyN6tVvOR1R1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "D_mel.shape"
      ],
      "metadata": {
        "id": "AniAq1mND-GJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the DCT\n",
        "import scipy\n",
        "n_mfcc=20\n",
        "M = scipy.fftpack.dct(D_mel, axis=-2, norm=\"ortho\")[..., :n_mfcc, :]"
      ],
      "metadata": {
        "id": "kKEV-_f1aRND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finally, we get our MFCCs\n",
        "M.shape"
      ],
      "metadata": {
        "id": "yLQe1M9DS4j0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "librosa.display.specshow(M)"
      ],
      "metadata": {
        "id": "_fXZ28MA8joo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's another well-documented extraction [function](https://github.com/jameslyons/python_speech_features/blob/master/python_speech_features/base.py)."
      ],
      "metadata": {
        "id": "O1C8Qi_YTlEE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXxyQOriwasM"
      },
      "source": [
        "### Delta MFCC features\n",
        "\n",
        "Sound is a moving signal. Features like MFCCs are only taken at static points in time. **Delta features** are used to measure the change in features between two frames. So for example, the delta MFCCs of two frames are simply the MFCCs of the second frame minus the MFCCs of the first frame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2H7IynotZSA"
      },
      "source": [
        "mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)\n",
        "delta = librosa.feature.delta(mfcc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGq1a8QItjY_"
      },
      "source": [
        "print(mfcc)\n",
        "print(delta)\n",
        "print(mfcc.shape)\n",
        "print(delta.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9koYLqaCYup8"
      },
      "source": [
        "## Other features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10UbXHWSTros"
      },
      "source": [
        "### Pitch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IteADohpQxQd"
      },
      "source": [
        "Pitch is the subjective human perception of the fundamental frequency (F0).\n",
        "\n",
        "The higher the fundamental frequency, the higher the pitch.\n",
        "\n",
        "The relationship between the fundamental frequency and pitch is not linear because our hearing does not represent frequency differences below 100Hz and above 1000Hz accurately.\n",
        "\n",
        "For example, to the human ear, the difference between 900Hz and 1000Hz sounds larger than the difference between 1000Hz and 1100Hz.\n",
        "\n",
        "Between 100Hz and 1000Hz, the fundamental frequency and pitch correlate linearly. Above, they correlatey logarithmically.\n",
        "\n",
        "Since pitch is subjective, it can't be measured objectively in the way we measure physics-based sound properties."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gB_R94VbW5SL"
      },
      "source": [
        "#### Pitch Extraction\n",
        "\n",
        "The features we discussed above relate to the amplitude and occur at the sample level. They are time-domain audio features.\n",
        "\n",
        "Pitch is measured at the FFT (Fast Fourier Transform) frame level. It is a frequency-domain audio feature.\n",
        "\n",
        "These frames are applied to the audio track at very small, overlapping intervals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPHYjWooW8Os"
      },
      "source": [
        "##### Librosa\n",
        "\n",
        "Librosa has an algorithm for tracking pitch. Unfortunately it doesn't work very well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pkMeB0F1U0Y"
      },
      "source": [
        "#Don't use this\n",
        "pitch_values, magnitudes = librosa.piptrack(y=y, sr=sr)#, fmin=75, fmax=600)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCJmaz2oY1K0"
      },
      "source": [
        "pitch_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAhATlF0YdPK"
      },
      "source": [
        "import numpy as np\n",
        "with np.printoptions(threshold=np.inf):\n",
        "    print(pitch_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Au8uGbz2W9Hs"
      },
      "source": [
        "##### Parselmouth/Praat\n",
        "\n",
        "Praat has its own [function](https://www.fon.hum.uva.nl/praat/manual/Sound__To_Pitch___.html) for pitch tracking, and at least for this audio track, it works better than librosa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKPBLMWoxIEo"
      },
      "source": [
        "!pip install praat-parselmouth"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzfC_l3KxbdM"
      },
      "source": [
        "import parselmouth\n",
        "snd = parselmouth.Sound(\"i.wav\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsWYHKlXQzWM"
      },
      "source": [
        "#Use this\n",
        "pitch = snd.to_pitch()\n",
        "pitch_values = pitch.selected_array['frequency']\n",
        "pitch_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9w0tN6e0ctRO"
      },
      "source": [
        "If you are serious about pitch detection, you might also want to consider [Matlab](https://www.mathworks.com/help/audio/ref/pitch.html). Wesleyan students get to use it for free and you can also use an online version at https://myapps.wesleyan.edu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9J5xybRXF36"
      },
      "source": [
        "##### Exercise\n",
        "\n",
        "1. Measure the pitch\n",
        "2. Use Audacity to increase the pitch by 30%\n",
        "3. Measure the pitch again and compare\n",
        "\n",
        "Use the following audio clip:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87fYm1WwMy7b"
      },
      "source": [
        "!gdown https://drive.google.com/uc?id=1-9zXUFVRdUMPltiN8pPcydBd8SJr93Sy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snd = parselmouth.Sound(\"harris_speech.wav\")\n",
        "pitch = snd.to_pitch()\n",
        "pitch_values = pitch.selected_array['frequency']\n",
        "pitch_values"
      ],
      "metadata": {
        "id": "p56BOPKV_NwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7HpcPICS_1p3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "snd = parselmouth.Sound(\"/content/drive/Shareddrives/qac239_prep/Audio_Section_2022_Fall/Audio_Day4/harris_speech_pitch.wav\")\n",
        "pitch = snd.to_pitch()\n",
        "pitch_values2 = pitch.selected_array['frequency']"
      ],
      "metadata": {
        "id": "LowgOKMv_YGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratios = pitch_values2/pitch_values\n",
        "ratios = ratios[np.isfinite(ratios)]\n",
        "ratios = ratios[ratios != 0]\n",
        "ratios"
      ],
      "metadata": {
        "id": "iptgNcd8AOal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.mean(ratios))"
      ],
      "metadata": {
        "id": "W25t0lp4A2e2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQqRcZCqXYAM"
      },
      "source": [
        "Examples for the use of pitch in social science research:\n",
        "- Politicians who speak with lower pitch are more popular with voters ([Klofstad, Anderson & Peters 2012](https://royalsocietypublishing.org/doi/10.1098/rspb.2012.0311), [Klofstad 2016](https://web.as.miami.edu/personal/cklofstad/25_polpsych_pitch.pdf)).\n",
        "- Using pitch to predict the votes of Supreme Court justices ([Dietrich, Enos & Sen 2018](https://www.cambridge.org/core/journals/political-analysis/article/emotional-arousal-predicts-voting-on-the-us-supreme-court/1047BF7D73A1B45BDB1C61A3A80E0F64)).\n",
        "- Using pitch to detect which topics a political speaker cares about and wants to emphasize the most ([Dietrich, Hayes, O'Brien 2019](https://www.cambridge.org/core/journals/american-political-science-review/article/pitch-perfect-vocal-pitch-and-the-emotional-intensity-of-congressional-speech/868A227A2C57A7053742CF9CF3B6C419))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJj4yYn2TfIn"
      },
      "source": [
        "### Measures of loudness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHsg2UlPU_R1"
      },
      "source": [
        "#### Root-mean-square amplitude"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4japFH6hvi3Q"
      },
      "source": [
        "To measure the amplitude (as a measure of loudness) over a given interval, root-mean-square amplitude can be used. Rather than just taking the average amplitude, which would result in a value close to 0 because the positive and negative amplitude values would cancel each other out, we square them and then divide them by the number of samples. Then we take the root."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NX2bvzuqvOR7"
      },
      "source": [
        "$N$ = number of samples \\\\\n",
        "$i$ = sample i \\\\\n",
        "$x_i$ = amplitude at sample i"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QqM_RzNem09"
      },
      "source": [
        "RMS amplitude $= \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N}x^2_i}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reQB7bgr1Lgc"
      },
      "source": [
        "#Librosa allows us to measure RMS amplitude like this:\n",
        "librosa.feature.rms(y=y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Librosa calculates RMS amplitude for each frame individually. In the [source code](https://librosa.org/doc/latest/_modules/librosa/feature/spectral.html#rms), the signal is divided into frames like this: \\\\\n",
        "` x = util.frame(y, frame_length=frame_length, hop_length=hop_length)`\n",
        "\n",
        "Beyond that, it is effectively the same formula we used (except that there is a mean for each frame): \\\\\n",
        "`np.sqrt(np.mean(np.abs(x) ** 2, axis=-2, keepdims=True))`"
      ],
      "metadata": {
        "id": "nI_KvcR5LdJF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZhYN-IKZPmQ"
      },
      "source": [
        "Exercise:\n",
        "\n",
        "Using the formula for RMS amplitude, implement the math yourself for `y`, and compare it to the output of the librosa function.\n",
        "\n",
        "RMS amplitude $= \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N}x^2_i}$"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "np.sqrt(sum(y**2)/len(y))"
      ],
      "metadata": {
        "id": "NlF8tlgSDDby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P21DmAUl0RWv"
      },
      "source": [
        "#### Zero-crossing rate\n",
        "Zero-crossing rate measures the number of times in a given time interval/frame that the amplitude of the speech signals passes through zero.\n",
        "\n",
        "silence vs. any signal; i.e. voice activity detection\n",
        "\n",
        "EEG\n",
        "\n",
        "![](https://ars.els-cdn.com/content/image/3-s2.0-B9780080993881000042-f04-04-9780080993881.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0aCegQD0VXg"
      },
      "source": [
        "#### Chroma features\n",
        "12-element vector representation of the spectral energy for each pitch class in music: C, C#, D, D#, E, F, F#, G, G#, A, A# and B. Mainly used for music classification. The standard deviation is called the Chroma deviation.\n",
        "\n",
        "Application: speech-music discrimination"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGbvNlNFbWM0"
      },
      "source": [
        "!wget https://upload.wikimedia.org/wikipedia/commons/b/b4/12tet_diatonic_scale.ogg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFzOyevNbYWv"
      },
      "source": [
        "import librosa\n",
        "import librosa.display\n",
        "y, sr = librosa.load(\"12tet_diatonic_scale.ogg\")\n",
        "out = librosa.feature.chroma_stft(y, sr)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "img = librosa.display.specshow(out, y_axis='chroma', x_axis='time', ax=ax)\n",
        "fig.colorbar(img, ax=ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmhAYMLYW2qu"
      },
      "source": [
        "#### Spectral entropy\n",
        "Shannon entropy of the power spectral density. Used to measure signal irregularity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5I1eCbS6WsXo"
      },
      "source": [
        "#### Spectral flux\n",
        "Squared difference between the normalized magnitudes of the spectra of the two successive short-term windows. Spectral flux tends to be higher for speech than for music. Useful for determining timbre to classify music instruments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yy1PMrsQYwkd"
      },
      "source": [
        "### Exercise\n",
        "1. Use librosa to calculate the zero-crossing rate of the diatonic scale audio.\n",
        "2. Calculate its delta features.\n",
        "3. Plot both over time and compare."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izCZtdwbyySO"
      },
      "source": [
        "## Homework\n",
        "\n",
        "Librosa contains a couple of example audio tracks, among them a song called 'Fishin'. You can import it like this:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "y, sr = librosa.load(librosa.example('fishin'))\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "1. Restrict the audio to the first 30 seconds of the song.\n",
        "2. Calculate the [spectral flux](https://librosa.org/doc/latest/generated/librosa.onset.onset_strength.html?highlight=spectral%20flux#).\n",
        "3. Calculate the running average spectral flux for windows of size 100. You can use the approach described [here](https://stackoverflow.com/a/54628145).\n",
        "4. Make a scatterplot with time, in seconds, on the x-axis, and running average spectral flux on the y-axis.\n",
        "5. Listen to the first 30 seconds of the song (you can export it to an audio file with `soundfile.write`). What do the peaks and valleys in the scatterplot correspond to?\n",
        "\n",
        "Turn in your notebook in the Moodle dropbox by Friday, November 25, 11:59pm."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import librosa.display\n",
        "import soundfile"
      ],
      "metadata": {
        "id": "qGKGgqQLPPT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y, sr = librosa.load(librosa.example('fishin'), duration = 30.0)\n",
        "sec30 = librosa.time_to_samples(np.array([0, 30]),sr)\n",
        "y = y[sec30[0]:sec30[1]]\n",
        "D = np.abs(librosa.stft(y))\n",
        "times = librosa.times_like(D)\n",
        "fig,ax = plt.subplots(nrows=2,sharex=True)\n",
        "librosa.display.specshow(librosa.amplitude_to_db(D, ref=np.max),y_axis='log', x_axis ='time', ax=ax[0])\n",
        "\n",
        "ax[0].set(title='Power spectrogram' )\n",
        "ax[0].label_outer()\n",
        "onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
        "ax[1].plot (times, 2 + onset_env / onset_env.max(), alpha=0.8, label= 'Mean (mel)')\n",
        "onset_env = librosa.onset.onset_strength(y=y, sr=sr, aggregate=np.median, fmax=8000, n_mels=256)\n",
        "ax[1].plot (times, 1 + onset_env / onset_env.max(), alpha=0.8, label='Median (custom mel)')\n",
        "C = np.abs (librosa.cqt(y=y,sr=sr) )\n",
        "onset_env = librosa.onset.onset_strength(sr=sr, S=librosa.amplitude_to_db(C, ref=np.max) )\n",
        "ax[1].plot(times, onset_env/ onset_env.max(), alpha=0.8, label='Mean (CQT)')\n",
        "ax[1].legend()\n",
        "ax[1].set(ylabel=' Normalized strength', yticks=[])"
      ],
      "metadata": {
        "id": "Xh3GB_Q4O5n0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ra = np.convolve(onset_env, np.ones (100) ,'valid') / 100\n",
        "sec = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
        "ra.size"
      ],
      "metadata": {
        "id": "si0XopNVzQg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(np.arange(0,ra.size), ra, s=None, c=None, marker=None, cmap=None, vmin=None, vmax=None, alpha=None, linewidths=None, edgecolors=None)"
      ],
      "metadata": {
        "id": "_bnWdwwLzgzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soundfile.write (librosa.example('fishin'), y,sr)"
      ],
      "metadata": {
        "id": "O7-8h0-Z0DPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The peaks align with the vocals and the valleys show where the audio is only instrumentals."
      ],
      "metadata": {
        "id": "njZd3SxF0OGu"
      }
    }
  ]
}